
\documentclass[12pt,a4paper,twoside]{article}

\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\begin{document}
\title{Text classification mit Machine Learning Methoden}
\author{Lukas Hofmaier \texttt{lukas.hofmaier@hsr.ch}}
\maketitle

\section{Das Text Classification Problem}
\label{sec:problem}

Dieser Abschnitt gibt erkl"art, worum es bei Text Classication geht.

Es gibt folgende Ansatze um eine Menge von Dokumenten in Klassen zu unterteilen:
\begin{description}
\item[Manuelle Klassifizierung]
\item[hand-craftet Rules]
\item[Machine learning-based] 
\end{description}


Bei Text Classification geht es darum, Dokumente vordefinierten Klassen zuzuordnen. Man m"ochte eine Menge von Dokumenten in Klassen unterteilen. Sucht man nur nach Dokumenten einer bestimmten Klasse, muss man nicht jedes Dokument manuell "uberpr"ufen, ob es relevant ist. Klassen k"onnen z.B. Themen sein. Man k"onnte die Dokumente zu einem Thema suchen, indem man einen boolschen Ausdruck definiert, der ausdr"uckt welche W"orter in einem Dokument enthalten sein m"ussen. Dieser Ansatz hat den Nachteil, dass es schwierig ist, geeignete boolsche Ausdr"ucke zu formulieren, die zu guten Resultaten f"uhren.

Eine anderer Ansatz ist machine learning-based Text Classification. Bei diesem Ansatz wird die Regel nach, der man Dokumente, den Klassen zuteilt, aus sogenannten Trainings Daten, berechnet. Die Trainingsdaten (training set $\mathbb{D}$ ) bestehen aus einer Menge von Dokumenten, denen bereits die richtige Klasse zugeordnet wurde. Diese Zuordnung kann z.B. manuell passieren. Dieser Ansatz wird auch supervised learning oder "uberwachtes Lernen genannt. 

Die generierte Regel, wird auch classifier oder classification function $\gamma$ genannt.
\[
\gamma : \mathbb{X} \to \mathbb{C}
\]

$\mathbb{X}$ ist der Document space und $\mathbb{C}$ ist die Menge aller Klassen. Der Classifier nimmt ein Dokument als Input und gibt die passende Klasse zur"uck. F"ur Dokumente gibt es unterschiedliche Repr"asentationen. Eine m"ogliche Repr"asentation wird in Abschnitt \ref{sec:vectorspacemodel} erkl"art.

Die Trainingsdaten sind der Input der Lernmethode $\Gamma$. $\Gamma$ gibt die classification function $\gamma$ zur"uck.
\[
\Gamma(\mathbb{D}) = \gamma
\]



\section{Linear classifiers}
\label{sec:linearclassifiers}

\subsection{Vector space model}
\label{sec:vectorspacemodel}
Die Classification function $\gamma$ bestimmt f"ur ein gegebenes Dokument $d$ die Klasse $c$. Bei den folgenden Classifier muss das Dokument als Vektor re\-pr"as\-entiert werden. Der folgenden Abschnitt beschreibt die Repr"asentation eines Dokumentes als Vektor.

Um Dokumente mit den nachfolgenden Methoden zu klassifizieren, wird jedes Dokument als Vektor gespeichert. Jedem Wort von Interesse wird ein Wert zugeordnet. Eine einfache M"oglichkeit, w"are die Anzahl des Auftretens f"ur jedes Wort in dem Vektor abzuspeichern. Diese Repr"asentation ber"ucksichtig nicht, dass allgemeine W"orter wie z.B. "Auto", wenig "uber den Inhalt eines Dokumentes aussagen, aber trotzdem, oft vorkommen.

Um die Seltenheit eines Wortes zu bestimmen, wird die Anzahl der Dokumente, die das Wort enthalten bestimmt. Wenn $N$ die Anzahl aller Dokumente ist, kann mit der Formel

\[
idf_t = log \frac{ N}{df_f}
\]

die Seltenheit des Wortes bestimmt werden. Der Wert $idf_t$ zu einem Wort $t$ ist umso h"oher umso seltener ein Wort in einer Menge von Dokumenten vorkommnt. Man nennt diesen Wert inverse document frequency\cite{manning08}.

Die Anzahl des Auftretens eines Wortes $t$ innerhalb eines Dokumentes $d$ wird term frequency $tf_{t,d}$ genannt.

Die Werte $tf$ und $idf$ werden zu dem sogenannten $tf-idf$ Wert kombiniert. Dieser berechnet sich f"ur ein Wort $t$ in einem Dokument $d$ wie folgt:
\[
tf-idf_{t,d} = tf_{t,d} \times idf_t
\]

\begin{table}
  \centering
  \begin{tabular}{l l l l l l}
    & Dok1 & Dok2 & Dok3 & idf & tf-idf Dok1 \\
    Chip & 27 & 4 & 24 & 1.65 &44.5  \\
  \end{tabular}
  \caption{tf-idf Beispiel Werte }
  \label{tab:temporal_operators}
\end{table}

Bei Text classication geht es darum, "ahnliche Dokumente zu finden. Jedem Wort in einem Dokument kann der $tf-idf$ - Wert zugewiesen werden, dass heisst jedes Dokument kann als Vektor, dessen Komponenten $tf-idf$ Werte sind repr"asentiert werden. Werden alle Dokumente, die klassifiert werden, als tf-idf-Vektoren mit einem gemeinsamen Vektorraum dargestellt, spricht man vom vector space model.




\subsection{Vector space classification}
\label{sec:vectorclassification}

Wie in Abschnitt \ref{sec:vectorspacemodel} erk"art, k"onnen Dokumente als Vektoren dargestellt werden. Um die Funktionsweise der nachfolgenden Methoden zu erkl"aren, haben diese Vektoren zwei Komponente. Wenn es dabei um Vektoren mit zwei Komponenten handelt, k"onnen diese als Punkte in der Ebene dargestellt werden. In der Realit"at arbeitet man mit Vektoren die Punkte in einer Hyperebene darstellen.

Wenn alle Dokumente als Punkte in der Ebene dargestellt werden, de\-finiert ein linearer Klassifizierer eine Linie in dieser Ebene. Ein Algorithmus, der neue Dokumente klassifiziert, muss entscheiden, ob die Vektor space Repr"a\-sentation des Dokuments auf welcher Seite liegt. Die Linie wird auch decision boundary genannt. Die Herausforderung der text classification liegt darin, gute decision boundaries zu finden.

\todo{Bild von 2D Plane mit decision boundiary erstellen}

\subsubsection{Rocchio classification}
\label{sec:rocchio}

Rocchio classifiation ist ein m"ogliches Verfahren um Dokumente zu klassifizieren. Die Grenzen zwischen den Klassen werden mit sogenannten centroids definiert. Ein centroid ist eine Vektor, dessen Komponente die Durchschnitts\-werte aller Vektoren der selben Klasse darstellen.
\[
\vec \mu(c) = \frac{1}{|D_c|} \sum_{d \in D} \vec v (d)
\]
$\vec (d)$ ist die Vektor space Repr"asentation. $D_c$ ist die Menge aller Dokumente der Klasse, dessen centroid berechnet werden soll.

Neue Dokumente werden nach folgender Regel zugeordnet: Es wird die tdf-idf repr"asentation $\vec v (d)$ f"ur das neue Dokument $d$ berechnet. Dann wird $d$ derjenigen Klasse zugeordnet zu dessen centroid $\vec v (d)$ die geringste Distanz hat.

\begin{algorithm}
\caption{Rocchio Trainings-Algorithmus}
\begin{algorithmic}

\ForAll{ $c_i \in $ Klasses} 
\State $ D_j \gets  \{d : (d,c_j) \in$ Classes $\}$
\State  $\vec \mu_j \gets \frac{1}{|D_c|} \sum_{d \in D} \vec v (d)$
\EndFor

\Return ${\vec \mu_1, \dots , \vec \mu_n }$    
 \end{algorithmic}  
\end{algorithm}

\section{Support vector machines}
\label{sec:svm}


Gegeben ist eine Menge von Punkten die man in zwei Gruppen aufteilen m"ochte.  Die Kategorien oder Gruppen werden getrennt durch eine Hyperebene.

In diesem Abschnitt wird die Methode Support Vector Machines ( SVM) f"ur Text Classiciation vorgestellt. Wie in Abschnitt \ref{vectorclassification} erkl"art, definiert ein Klassifizierer eine Hyperebene. Diese ist definiert durch den Achsenabschnitt $b$ und den Normalenvektor $ \vec w $. Alle Punkte $\vec x $ die auf dieser Hyperebene liegen, erf"ullen die Gleichung 
\[
\vec w^T \vec x = -b
\].

Um neue Dokumente k"onnen wie folgt klassifiziert werden: Die Funktion
\[
f(\vec x ) = sign( \vec w^T \vec x + b)
\]
wird ausgewertet. Je nachdem, ob der Wert $1$ oder $-1$ ist wird das Dokument einer Klasse zugeordnet.

\begin{thebibliography}{99}
\bibitem{manning08}
Christopher D.Manning Prabhakar Raghavan Hinrich Sch"utze,
Introduction to Information Retrieval,
Cambridge University Press,
2008.
\end{thebibliography}


\end{document}
